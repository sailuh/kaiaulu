---
title: "OpenHub API Interfacing for Project Search"
output: 
  html_document:
    toc: true
    number_sections: true
vignette: >
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteIndexEntry{OpenHub API Interfacing for Project Search}
  %\VignetteEncoding{UTF-8}
---

# Introduction

## Motivation

This notebook showcases how to use Kaiaulu's OpenHub API interface functions to facilitate selecting open-source projects for studies. These OpenHub API interfacing functions return data tables that contain project information such as total contributor count, total lines of code, percentage breakdown of code languages, number of contributors who made at least one commit in the past year, the number of commits in the past year, etc. The current search criteria: the organization to which the projects belong and the primary programming language used by the projects.

The notebook is unique among its peers in that it concerns itself not with one project, but their selection, and thus it does not need to use the project configuration file architecture nor the project initialization.

## Purpose

This notebook explains how to acquire information on a set of projects (e.g. LOC on the current date, number of contributors who made at least one commit in the past 12 months, number of commits in the past 12 months, total commit count on the current date, and total number of contributors on the current date) that reside in [Openhub's open-source project collection](https://openhub.net/explore/projects) based on search parameters under an [organization](https://openhub.net/explore/orgs) using [Ohloh API](https://github.com/blackducksoftware/ohloh_api).

Kaiaulu's interface to Ohloh's API, an API for OpenHub's open-source project collection, relies on [httr](https://httr.r-lib.org) to create http GET requests that interface with Ohloh's API. Ohloh API responds to these requests by returning an XML response file with nested tags.

Kaiaulu only defines a few API endpoints of interest ([organization](https://github.com/blackducksoftware/ohloh_api/blob/main/reference/organization.md), [portfolio_projects](https://github.com/blackducksoftware/ohloh_api/blob/main/reference/portfolio_projects.md), [project](https://github.com/blackducksoftware/ohloh_api/blob/main/reference/project.md), and [analysis](https://github.com/blackducksoftware/ohloh_api/blob/main/reference/analysis.md)) where the tool is currently used, and parses the returned XML output into a table keeping only fields of interest. More endpoints and/or fields of interest per endpoint can be added in the future.

## Create a Personal API Token

OpenHub has a limit to the number of API calls per token (maximum set to 1000) per day. The current rate for your personal API token and to acquire an API key, they may be found at [its website](https://www.openhub.net/accounts/me/api_keys) and an account is required to acquire your personal API token. The process, **to create an account and register for an API key is documented on [Ohloh API GitHub Page API Section](https://github.com/blackducksoftware/ohloh_api?tab=readme-ov-file#api-key)**, should not take more than two minutes.

The functions in Kaiaulu will assume you have an OpenHub API token available, which can be passed as parameter. 

## Libraries

Please ensure the following R packages are installed on your computer. 

```{r warning = FALSE, message = FALSE}
rm(list = ls())
require(kaiaulu)
require(stringi)
require(data.table)
require(knitr)
require(httr)
require(gt)
```

# Configuration Section

## Selecting an OpenHub Organization

To start the project selection process, we may want to restrict ourselves to focusing on projects under a specific organization. The [list of organizations are found on the OpenHub website](https://openhub.net/explore/orgs). For this notebook, we will focus on searching for projects with their primary language as Java under the ["Apache Software Foundation"](https://openhub.net/orgs/apache).

Below are a set of required variables for the `openhub_*` functions.

```{r}
study_name <- "placeholder"
html_url_or_name <- "https://openhub.net/orgs/apache"
language <- "java"
token <- scan("~/.ssh/openhub_token",what="character",quiet=TRUE)
```

Explanation:

* study_name: The name of the study for project selection that will be used to create a folder system to store API responses.
* html_url_or_name: Either the URL for the organization page on OpenHub (e.g. "https://openhub.net/orgs/apache") or the short, unique, handle for the organization (e.g. "apache").
* language: The code language to filter for projects only containing the specified code language (e.g. "java").
* token: The file named "openhub_token" containing the OpenHub API Token.

## API Response Folder Storage

A hardcoded function for folder creation to store the API responses from each endpoint:

```{r}
openhub_folder_create <- function(folder_path, folder_name) {
  folder <- file.path(folder_path, folder_name)
  if (!dir.exists(folder)) {
    folder <- io_make_folder(folder_path, folder_name)
  }
  return(folder)
}
github_folder <- dirname(dirname(getwd()))
rawdata_folder <- openhub_folder_create(github_folder, "rawdata")
openhub_folder <- openhub_folder_create(rawdata_folder, "openhub")
study_folder <- openhub_folder_create(openhub_folder, study_name)
organization_folder_path <- openhub_folder_create(study_folder, "organization/")
portfolio_project_folder_path <- openhub_folder_create(study_folder, "portfolio/")
project_folder_path <- openhub_folder_create(study_folder, "project/")
analysis_folder_path <- openhub_folder_create(study_folder, "analysis/")
```

# Collecting and Parsing Data via Ohloh API

In this section, for each endpoint, we collect the data through a series of Ohloh API requests, and parse the API responses with its corresponding parser function. These parsed API responses are data tables which are displayed for each subsection. The values from one endpoint may be extracted for use to obtain a path to the next endpoint, and the merging of data tables is important for a holistic display of the data for the list of projects.

## Organizations

We call `openhub_api_organizations` to acquire the organization API response of our selected organization specified by `html_url_or_name`. This function downloads the API response file into the folder specified by `organization_folder_path`.

```{r, eval = FALSE}
openhub_api_organizations(token, organization_folder_path, html_url_or_name)
```

With the organization API response (only one page), we may parse this response with its corresponding parser function, `openhub_parse_organizations`, to acquire a data table with columns representing the tags for the organization listed:

* name: The name of the organization.
* html_url_projects: The URL to the organization on OpenHub's website corresponding to a list of portfolio projects for the organization.
* portfolio_projects: The number of portfolio projects under the organization.

```{r, eval = FALSE}
openhub_organization_api_requests <- openhub_retrieve(organization_folder_path)
openhub_organizations <- openhub_parse_organizations(openhub_organization_api_requests)
gt(openhub_organizations)
```

We then select the first organization's "html_url" column value (e.g. "https://openhub.net/orgs/apache") and filter to extract the short, unique handler for the organization (e.g. "apache"). 

```{r, eval = FALSE}
org_html_url <- openhub_organizations[["html_url"]][[1]]
org_name <- stringi::stri_extract_last_regex(org_html_url, "[^/]+$")
```

## Portfolio Projects

Following a similar process as the Organization section, we acquire the portfolio projects for the organization, "Apache Software Foundation", that possess the code language specified by `language`, in this case Java, by acquiring the portfolio projects API response files and parsing these API requests into a data table. This endpoint, portfolio projects, is iterable by pages, so we may use `openhub_api_iterate_pages` on `openhub_api_portfolio_projects` to acquire each page, storing these API responses in the folder path `portfolio_project_folder_path`. Currently, **each page for the portfolio_projects collection returns a maximum of 20 items**, portfolio projects. To grab as many matches as possible or up to a number of pages (if `max_pages` exceeds the total pages available by the API response, it will grab the maximum number of pages possible), the `max_pages` may be removed from `openhub_api_iterate_pages` or set to a desired value (e.g. "max_pages=1" returning up to 20 portfolio project items and "max_pages=16" returning up to 320 portfolio project items), respectively.

```{r, eval = FALSE}
openhub_api_iterate_pages(token, openhub_api_portfolio_projects, portfolio_project_folder_path, org_name, max_pages=NULL)
```

With the portfolio project API responses saved, we may parse these responses with its corresponding parser function, `openhub_parse_portfolio_projects`, to acquire a data table with columns representing the tags for each portfolio project listed. To narrow down the list of projects that the next endpoint will require (to save on API requests), we filter the data table of portfolio projects to show only projects that have Java, specified by `language`, as their primary code language.

* name: The name of the portfolio project.
* primary_language: The primary code language used by the portfolio project.
* activity: The portfolio project's activity level (Very Low, Low, Moderate, High, and Very High).

```{r, eval = FALSE}
portfolio_projects_api_requests <- openhub_retrieve(portfolio_project_folder_path)
openhub_portfolio_projects <- openhub_parse_portfolio_projects(portfolio_projects_api_requests)
openhub_portfolio_projects <- openhub_portfolio_projects[primary_language == language]
gt(openhub_portfolio_projects)
```

## Projects

To acquire more specific information about a portfolio project, we need to access it in the project collection, and the link between the portfolio_project endpoint and project endpoint is the "name" tag (e.g. "Apache Tomcat"). Project names on OpenHub are unique. Following a similar style of acquiring the project API responses and parsing them with its corresponding parser function, we loop through each "name" column in the portfolio projects' data table `openhub_portfolio_projects`. For each project name, `project_name`, we gather its API responses by using `openhub_api_iterate_pages` on `openhub_api_projects`, storing these API response files in the folder path `project_folder_path`, and specifying `max_pages` to 1 (Using the API's collection request query command, the first API requested page will contain a project with a matching "name" tag, thus there is no need to waste API calls to search through the other pages for the project, so `max_pages` is set to 1). The collection request query command may return, in a single project API response file page, multiple project items, this is due to the limitation of OpenHub's API query command, which performs a type of "ctrl+F" find search across all tags, instead of allowing users to specifically query a tag for an exact match.

```{r, eval = FALSE}
for (i in 1:nrow(openhub_portfolio_projects)) {
  project_name <- openhub_portfolio_projects[["name"]][[i]]
  openhub_api_iterate_pages(token, openhub_api_projects, project_folder_path, project_name, max_pages=1)
}
```

With the project API responses saved, we may parse these responses with its corresponding parser function, `openhub_parse_projects`, to acquire a data table with columns representing the tags for each project listed.

* name: The name of the project.
* id: The project's unique ID.
* html_url: The project's url to the current Project's details page on OpenHub.
* mailing_list: The project's mailing list url link (if "N/A", please check the project's url (html_url) and verify under the links section to verify that the project doesn't have a mailing list to be certain. The parser, `openhub_parse_projects`, is overloaded to read each link and uses a regular expression pattern to decipher the links to acquire the mailing list).

```{r, eval = FALSE}
projects_api_requests <- openhub_retrieve(project_folder_path)
openhub_projects <- openhub_parse_projects(projects_api_requests)
gt(openhub_projects)
```

We combine the portfolio_projects and project data tables into one data table, `openhub_combined_projects`, by performing an inner-join by "name" column. We add an additional filter, unique by "id" column, to ensure no duplication due to the projects endpoint possibly returning duplicated project items from the OpenHub API's collection request query command.

```{r, eval = FALSE}
openhub_combined_projects <- unique(merge(openhub_projects, openhub_portfolio_projects, by = "name", all = FALSE), by = "id")
gt(openhub_combined_projects)
```

## Analyses

The previously acquired "id" tag (represented as a column) for each project allows us to acquire the latest analysis collection for a project, containing a multitude of important metrics. Following the same logic as the Projects section, looping through each project in `openhub_combined_projects`, we call `openhub_api_analyses` to acquire the analysis API response of our selected project specified by `project_id`. This function downloads the API response file into the folder specified by `analysis_folder_path`. In addition, we pass the name of the project to `openhub_api_analyses` for use in the file name of the stored API response file. **The attached timestamp in the file's name for the analysis API response files are NOT timestamps for when the API response file was requested, but rather they correspond to the timestamp on OpenHub's database for when the analysis collection was last calculated for that specific project**.

```{r, eval = FALSE}
for (i in 1:nrow(openhub_combined_projects)) {
  project_name <- openhub_combined_projects[["name"]][[i]]
  project_id <- openhub_combined_projects[["id"]][[i]]
  openhub_api_analyses(token, analysis_folder_path, project_id, project_name)
}
```

With the analysis API response (only one page), we may parse this response with its corresponding parser function, `openhub_parse_analyses`, to acquire a data table with columns representing the tags for each analysis listed.

* id: The project's unique ID.
* min_month: OpenHub's first recorded year and month of the project's data (typically the date of the project's first commit, YYYY-MM format).
* twelve_month_contributor_count: The number of contributors who made at least one commit to the project source code in the past twelve months.
* total_contributor_count: The total number of contributors who made at least one commit to the project source code since the project's inception.
* twelve_month_commit_count: The total number of commits to the project source code in the past twelve months.
* total_commit_count: The total number of commits to the project source code since the project's inception.
* total_code_lines: The most recent total count of all source code lines.
* code_languages: A language breakdown with percentages for each substantial (as determined by OpenHub, less contributing languages are grouped and renamed as "Other") contributing language in the project's source code (The parser, `openhub_parse_analyses`, is overloaded to read each coding language and uses stringi to combine these percentages of code languages for the project).

```{r, eval = FALSE}
analyses_api_requests <- openhub_retrieve(analysis_folder_path)
openhub_analyses <- openhub_parse_analyses(analyses_api_requests)
#openhub_analyses <- unique(openhub_analyses, by = "id")
gt(openhub_analyses)
```

## Combining the Data

Lastly, we combine the combined portfolio_projects and project data table, `openhub_combined_projects`, with the analysis data table, `openhub_analyses`, into one data table, `openhub_combined_data`, by performing an inner-join by "id" column.

```{r, eval = FALSE}
openhub_combined_data <- merge(openhub_combined_projects, openhub_analyses, by = "id", all = FALSE)
gt(openhub_combined_data)
```

# Relevant Information

## Commit Issue Coverage

If you're interested in verifying if a project labels their commits with issue IDs and whether they have unique issue types (i.e. "bug", "feature", "security bug", "refactoring", etc), which is outside of the scope of this notebook, please review this short explanation:

To check the issue IDs, it requires you to parse the project's code git log. Then you can use [this function](http://itm0.shidler.hawaii.edu/kaiaulu/reference/commit_message_id_coverage.html) on the resulting table. See [this notebook](http://itm0.shidler.hawaii.edu/kaiaulu/articles/bug_count.html#identifying-issue-ids-in-commit-messages) for example usage. This notebook uses the regex written in the project configuration file, which is a regex. The user will need to manually figure out from the git log if any can be found, to then specify in Kaiaulu configuration file, to then have Kaiaulu calculate the metric. There is no other way to automate that since the conventions used vary across projects, if at all used.
