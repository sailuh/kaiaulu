---
title: "Social Smells Showcase"
output: 
  html_document:
    toc: true
    number_sections: true
vignette: >
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteIndexEntry{Dv8 Showcase}
  %\VignetteEncoding{UTF-8}
---

# Introduction

This notebook explains how to compute social smells for a given open source project. Before we begin, it is important to understand what data is required to compute social smells, what Kaiaulu will do and also what will _not_ do for you. 

Social smell metrics requires both collaboration and communication data. Collaboration data is extracted from version control systems, while communication data can be obtained from whatever the project of interest uses for developers to communicate. 

Obtaining collaboration data is relatively painless: You need only clone the version control system locally. Currently Kaiaulu only supports analysis in Git, but in the future it may support other version control systems. Obtaining communication data, however, requires more effort on your part **depending on the project you choose**. This is because there is a large variety of communication mediums and archive types open source projects use.

Broadly, open source projects use mailing lists, issue tracker (comments), or both for communication. 

Example of mailing list archive types are [GNU Mailing List Manager](http://www.list.org/), [Google Groups](https://groups.google.com/), [Mail Archive](https://www.mail-archive.com/), [Apache's MOD Mbox](https://httpd.apache.org/mod_mbox/), [Free Lists](https://www.freelists.org/), [Discourse](https://www.discourse.org/), etc. On March 2021, even GitHub launched its own built-in communication medium, [Discussions](https://github.blog/2021-03-09-github-discussions-now-available-for-private-repositories/). 

Examples of issue tracker types include [JIRA](https://www.atlassian.com/software/jira), GitHub's built-in Issue Tracker, GitLab's Issue Tracker, Monorail (used in Google's Chromium), [Trac](https://trac.edgewall.org/), etc. You may also be interested in including discussion that occurs in GitHub's pull requests / GitLab merge requests. Issue tracker communication is currently **not** supported in Kaiaulu, but we plan to support GitHub Issue Tracker, Github Pull Requests, and JIRA in the future. 


It is of course not viable for Kaiaulu to implement interfaces to every single archive type out there. Therefore, to calculate social smells, we expect **you** can obtain a **.mbox** representation of the mailing list of interest. This may be available from the open source project directly (e.g. Apache projects mod_mbox interface makes it easier to crawl mbox files, which we have available a crawler for them), or via a crawler someone already made. For example, [gg_scraper](https://gitlab.com/mcepl/gg_scraper) outputs a mbox file from Google Group mailing lists (although it can only obtain partial information of the e-mails, as Google Groups truncate them, which may pose limitations to some steps of the analysis of the identity matching discussed below). In the future, we plan to also support pipermail, the archive type for GNU Mailing List Manager. 

The bottom line is, the required effort to obtain the mailing list data will vary depending on the open source project of interest, as open source projects may even transition over time through different archive types. Once you have available in your computer both git log, and an .mbox file, you are ready to proceed with the social smells analysis of this notebook. 

# Libraries

Please ensure the following R packages are installed on your computer. The dependency to igraph is not strictly required, but it will be used in this Notebook.

```{r warning = FALSE, message = FALSE}
rm(list = ls())
seed <- 1
set.seed(seed)

require(kaiaulu)
require(stringi)
require(data.table)
require(knitr)
require(igraph)
```

# Project Configuration File (Parameters Needed)

The parameters necessary for analysis are kept in a project configuration file to ensure reproducibility. In this project, we will use the [Apache's APR open source project](https://apr.apache.org/). Refer to the `conf/` folder on Kaiaulu's git repository for APR and other project configuration files. It is in this project configuration file we specify where Kaiaulu can find the git log and .mbox file from APR. We also specify here filters of interest: For example, if Kaiaulu should ignore test files, or anything that is not source code.

We also provide the path for `tools.yml`. Kaiaulu does not implement all available functionality from scratch. Conversely, it will also not expect all dependencies to be installed. Every function defined in the API expects as parameter a filepath to the external dependency binary. Tools.yml is a convenience file that stores all the binary paths, so it can be set once during setup and reused multiple times for analysis. You can find an example of tools.yml on the github repo from Kaiaulu root directory.

```{r}
tools_path <- "../tools.yml"
conf_path <- "../conf/apr.yml"

tool <- yaml::read_yaml(tools_path)
conf <- yaml::read_yaml(conf_path)

perceval_path <- tool[["perceval"]]
git_repo_path <- conf[["data_path"]][["git"]]
start_commit <- conf[["interval"]][["window"]][["start_commit"]]
end_commit <- conf[["interval"]][["window"]][["end_commit"]]
window_size <- conf[["interval"]][["window"]][["size_days"]]
mbox_path <- conf[["data_path"]][["mbox"]]
scc_path <- tool[["scc"]]


# Filters
file_extensions <- conf[["filter"]][["keep_filepaths_ending_with"]]
substring_filepath <- conf[["filter"]][["remove_filepaths_containing"]]
```

The remainder of this notebook does not require modifications. If you encounter an error in any code block below, chances are one or more parameters above have been specified incorrectly, or the project of choice may have led to an outlier case. Please open an issue if you encounter an error, or if not sure post on discussions in Kaiaulu's GitHub. E-mailing bugs is discouraged as it is hard to track. 

# Parsing Input Data

As stated in the introduction, we need both git log and mailing list to compute social smells. Therefore, the first step is to parse the raw data.

## Parse Gitlog

To get started, we use the `parse_gitlog` function to extract a table from the git log. You can inspect the `project_git` variable to inspect what information is available from the git log. 

```{r}
project_git <- parse_gitlog(perceval_path,git_repo_path)
project_git <- project_git  %>%
  filter_by_file_extension(file_extensions,"file")  %>% 
  filter_by_filepath_substring(substring_filepath,"file")
project_git <- project_git[order(data.AuthorDate)]
```

## Parse Mbox

Next, we parse the .mbox data using the `parse_mbox` function. Similarly to `parse_gitlog`, the returned object is a table, which we can inspect directly in R to see what information is available.

```{r}
project_mbox <- parse_mbox(perceval_path,mbox_path)
project_mbox <- project_mbox[order(data.Date)]
#project_mbox <- project_mbox[data.Date >= start_date & data.Date <= end_date]
```

# Smells

Having parsed both git log and mbox, we are ready to start computing the social smells. Social smells are computed on a "time window" granularity. For example, we may ask "between January 2020 and April 2020, how many organizational silos are identified in APR?". This means we will inspect both the git log and mailing list for the associated time period, perform the necessary transformations in the data, and compute the number of organizational silos. 

So we begin by specifying how large our time window should be, in days:

```{r}
window_size <- 90 # 90 days
```

Kaiaulu will then perform a non-overlapping time window of every 3 months of git log history and mailing list to identify the number of organizational silos, missing link and radio silence social smells. 

We "slice" the git log and mailing list tables parsed earlier in `window_size` chunks, and iterate in a for loop on each "slice". 

Within a slice we do the following: 

 1. Apply identity matching (the same person may have multiple identities) inter and intra sources.
 2. Construct a network representation of both the git log and the mailing list.
 3. Apply, if necessary, a projection of the networks (depending on the network function used)
 4. We apply community detection algorithms, necessary to calculate some of the social smells
 5. We compute the social smells
 
There is a large variety of customization in the above 5 steps. We will discuss them briefly here, as they directly impact the quality of your results. 

### Identity Matching

It is very common authors have multiple names and e-mails within and across git log, mailing lists, issue trackers etc. There is no perfect way to identify all identities of an individual, only heuristics. Kaiaulu has a large number of unit tests, each capturing a different example of how people use their e-mails. However, this is not magic. It is possible, and it has been encountered before, cases where a software is used in an open source project that may entirely compromise the analysis if done blind. For example, a common heuristic used for identity matching is to consider two accounts to have the same identity if either the First+Last name OR E-mail have an exact match. In one open source project we analyzed, we found a software was being used that masked all users with commit access as "admin@project.org". Using this heuristic would have, therefore, compromised the entire data.

Using the code below, you can store the various `project_git_slice` and `project_mbox_slice` to manually inspect in R the identity assigned to the various users. **I strongly encourage you to do so**. It is also possible to specify the `identity_match()` function to consider only names, instead of name and e-mails, to avert the example above. If you find a edge case where the identity is incorrectly assigned, please open an issue so we can add the edge case. You may also manually correct the identity numbers, before executing the remaining code blocks, to improve the accuracy of the results. 

Remember: Social smells rely heavily on patterns of collaboration and communication. If the identities are poorly assigned, the social smells will **not** reflect correctly the project status (since in essence several people considered to be communicating with one another, are the same individual!).

### Constructing a Network Representation

As mentioned in the introduction, there are multiple types of mailing list archives out there, and it may be more sensible to use an issue tracker instead of a mailing list, or a combination of both depending on the project. Besides data types, there are also different types of transformations that can be done, when we transform the data in networks. This notebook implements the bipartite transformation (see `parse_gitlog_network()`). It is also possible to use a temporal transformation (see `parse_gitlog_temporal_network()`). The choice of transformation impacts the direction and overall type of network that will be generated, so it is important you understand how this impact your research conclusions. A similar transformation could be applied to mailing lists, but it is not yet implemented. Because we use bipartite in the code block below, we also perform a bipartite projection. These are well known operations in graph theory which also impact the interpretability of the results.

Another transformation that you can choose is whether the analysis should be done on files, or entities (e.g. functions). See `parse_gitlog_entity_network()` and `parse_gitlog_entity_temporal_network()` for entities. You may choose functions, classes, and other more specific types of entities depending on the language of interest (e.g. typedef structs for C).  

A third choice we make here is whether the collaboration being analyzed is done by authors or committers. Normally a open source project has both. In the code block below, we analyze authors. If you are interested in committers, or potentially their interaction, see the available parameters of `parse_gitlog()`. 

### Community Detection

For some social smells, such as radio silence and primma donna, community detection is required to be applied to the constructed networks. Do consider the implications of the one chosen below in your results.


```{r}
# Define all timestamp in number of days since the very first commit of the repo 
# Note here the start_date and end_date are in respect to the git log.

# Transform commit hashes into datetime so window_size can be used
start_date <- get_date_from_commit_hash(project_git,start_commit)
end_date <- get_date_from_commit_hash(project_git,end_commit)
datetimes <- project_git$data.AuthorDate
mbox_datetimes <- project_mbox$data.Date

# Format time window for posixT
window_size_f <- stringi::stri_c(window_size," day")

# Note if end_date is not (and will likely not be) a multiple of window_size, 
# then the ending incomplete window is discarded so the metrics are not calculated 
# in a smaller interval
time_window <- seq.POSIXt(from=start_date,to=end_date,by=window_size_f)

# Create a list where each element is the social smells calculated for a given commit hash
smells <- list()
size_time_window <- length(time_window)
for(j in 2:size_time_window){
  i <- j - 1

  # If the time window is of size 1, then there has been less than "window_size_f"
  # days from the start date.
  if(length(time_window)  == 1){
    # Below 3 month size
    start_day <- start_date
    end_day <- end_date 
  }else{
    start_day <- time_window[i]
    end_day <- time_window[j] 
  }
  
  
  # Obtain all commits from the gitlog which are within a particular window_size
  project_git_slice <- project_git[(data.AuthorDate >= start_day) & 
                     (data.AuthorDate < end_day)]
  
  # Obtain all email posts from the mbox which are within a particular window_size
  project_mbox_slice <- project_mbox[(data.Date >= start_day) & 
                       (data.Date < end_day)]
  
  i_commit_hash <- first(project_git_slice)$data.commit
  j_commit_hash <- last(project_git_slice)$data.commit
  
  # Identity Matching
  project_log <- list(project_git_slice=project_git_slice,project_mbox_slice=project_mbox_slice)
  project_log <- identity_match(project_log,
                                name_column = c("data.Author","data.From"),
                                assign_exact_identity,
                                use_name_only=TRUE,
                                label = "raw_name"
                                )
  project_git_slice <- project_log[["project_git_slice"]]
  project_mbox_slice <- project_log[["project_mbox_slice"]]
      
  # Parse networks edgelist from extracted data
  project_git_slice <- parse_gitlog_network(project_git_slice,mode="author")
  project_mbox_slice <- parse_mbox_network(project_mbox_slice)
  
  # Load networks in igraph
  git_network <- igraph::graph_from_data_frame(d=project_git_slice[["edgelist"]], 
                      directed = TRUE, 
                      vertices = project_git_slice[["nodes"]])
  mbox_network <- igraph::graph_from_data_frame(d=project_mbox_slice[["edgelist"]], 
                      directed = TRUE, 
                      vertices = project_mbox_slice[["nodes"]])
  
  # Community Smells functions are defined base of the projection networks of 
  # dev-thread => dev-dev, and dev-file => dev-dev. This creates both dev-dev via graph projections
  git_network_authors <- igraph::bipartite_projection(git_network,
                                          multiplicity = TRUE,
                                          which = TRUE) # FALSE is the thread projection
  mbox_network_authors <- igraph::bipartite_projection(mbox_network,
                                          multiplicity = TRUE,
                                          which = TRUE) # FALSE is the thread projection
  # Community Detection
  mail.clusters <- walktrap.community(mbox_network_authors,
                                    weights=E(mbox_network_authors)$weight)
  code.clusters <- walktrap.community(git_network_authors,
                                    weights=E(git_network_authors)$weight)
  # Calculate Smells 
  org_silo <- length(community_smell_organizational_silo(mbox_network_authors,git_network_authors))
  missing_links <- length(community_smell_missing_links(mbox_network_authors,git_network_authors))
  radio_silence <- length(community_smell_radio_silence(mbox_network_authors, mail.clusters))
  primma_donna <- length(community_smell_primadonnas(mbox_network_authors,
                            mail.clusters,
                            git_network_authors))
  
  # Calculate Quality Framework Metrics
  st_congruence <- community_metric_sociotechnical_congruence(mbox_network_authors,git_network_authors)
  communicability <- community_metric_mean_communicability(mbox_network_authors,git_network_authors)
  
  commit_interval <- stri_c(i_commit_hash,"-",j_commit_hash)
  smells[[commit_interval]] <- data.table(commit_interval,
                                          org_silo,
                                          missing_links,
                                          radio_silence,
                                          primma_donna,
                                          st_congruence,
                                          communicability)
}
smells_interval <- rbindlist(smells)
```

# Other Metrics

The remainder of this notebook does not compute any social smells. It provide some popular metrics commonly reported in software engineering literature, which may be useful to you when interpreting the social smells. While their granularity is not at "time window" level, they are computed like so in order to be placed in the same table of social smells after being aggregated to the same granularity.

## Churn

```{r}
churn <- list()
for(j in 2:length(time_window)){
  i <- j - 1
  
  # If the time window is of size 1, then there has been less than "window_size_f"
  # days from the start date.
  if(length(time_window)  == 1){
    # Below 3 month size
    start_day <- start_date
    end_day <- end_date 
  }else{
    start_day <- time_window[i]
    end_day <- time_window[j] 
  }
  
  # Obtain all commits from the gitlog which are within a particular window_size
  project_git_slice <- project_git[(data.AuthorDate >= start_day) & 
                     (data.AuthorDate < end_day)]
  
  # The start and end commit
  i_commit_hash <- first(project_git_slice)$data.commit
  j_commit_hash <- last(project_git_slice)$data.commit
  
  # The start and end datetime
  start_datetime <- first(project_git_slice)$data.AuthorDate
  end_datetime <- last(project_git_slice)$data.AuthorDate
  
  commit_interval <- stri_c(i_commit_hash,"-",j_commit_hash)
  churn[[commit_interval]] <- data.table(commit_interval,
                                         start_datetime,
                                         end_datetime,
                                         churn=metric_churn_per_commit_interval(project_git_slice))
}
churn_interval <- rbindlist(churn)
```

## Line Metrics

```{r}
time_window <- seq.POSIXt(from=start_date,to=end_date,by=window_size_f)
#time_window <- seq(from=start_daydiff,to=end_daydiff,by=window_size)
line_metrics <- list()
for(j in 2:length(time_window)){
  i <- j - 1
  
  # If the time window is of size 1, then there has been less than "window_size_f"
  # days from the start date.
  if(length(time_window)  == 1){
    # Below 3 month size
    start_day <- start_date
    end_day <- end_date 
  }else{
    start_day <- time_window[i]
    end_day <- time_window[j] 
  }
  
  # Obtain all commits from the gitlog which are within a particular window_size
  project_git_slice <- project_git[(data.AuthorDate >= start_day) & 
                     (data.AuthorDate < end_day)]
  
  i_commit_hash <- first(project_git_slice)$data.commit
  # Use the ending hash of that window_size to calculate the flaws
  j_commit_hash <- last(project_git_slice)$data.commit
  # Checkout to commit of interest
  git_checkout(j_commit_hash, git_repo_path)
  # Run dv8 against the checkedout commit
  commit_interval <- stri_c(i_commit_hash,"-",j_commit_hash)
  line_metrics[[commit_interval]] <- parse_line_metrics(scc_path,git_repo_path)
  line_metrics[[commit_interval]]$commit_interval <- commit_interval
  line_metrics[[commit_interval]]$git_checkout <- j_commit_hash
  line_metrics[[commit_interval]] <- line_metrics[[commit_interval]][,.(commit_interval,
                                                                        git_checkout,
                                                                        Location,
                                                                        Lines,
                                                                        Code,
                                                                        Comments,
                                                                        Blanks,
                                                                        Complexity)]
  
  # Filter Files
  line_metrics[[commit_interval]] <- line_metrics[[commit_interval]]  %>%
  filter_by_file_extension(file_extensions,"Location")  %>% 
  filter_by_filepath_substring(substring_filepath,"Location")
}
# Reset Repo to HEAD
git_checkout("trunk",git_repo_path)

line_metrics_file <- rbindlist(line_metrics)
line_metrics_interval <- line_metrics_file[,.(Lines = sum(Lines),
                                 Code = sum(Code),
                                 Comments = sum(Comments),
                                 Blanks = sum(Blanks),
                                 Complexity = sum(Complexity)),
                              by = c("commit_interval","git_checkout")]
```


# Merge Churn, Smells, and Line Metrics

```{r}
dt <- merge(churn_interval,smells_interval,by="commit_interval")
dt <- merge(dt,line_metrics_interval,by="commit_interval")
kable(dt)
```

# Write Files

```{r eval = FALSE}
fwrite(dt,"~/Desktop/results_kaiaulu.csv")
```
