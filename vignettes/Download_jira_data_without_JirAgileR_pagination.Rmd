---
title: "Download JIRA Issues and Comments without JirAgileR with pagination"
output: 
  html_document:
    toc: true
    number_sections: true
vignette: >
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteIndexEntry{Download JIRA Issues and Commentss}
  %\VignetteEncoding{UTF-8}
---


```{r}
rm(list = ls())
seed <- 1
set.seed(seed)
```

```{r warning=FALSE,message=FALSE}
require(kaiaulu)
require(data.table)
#require(JirAgileR)
require(knitr, quietly = T)
require(dplyr, quietly = T)
require(jsonlite)
```

# Introduction

This example is adapted from the JirAgileR package [README.md](https://github.com/matbmeijer/JirAgileR). 

As usual, the first step is to load the project configuration file. 

# Project Configuration File

In this notebook, we will obtain data from the issue tracker JIRA. We will use [Apache's Geronimo open source project](https://geronimo.apache.org). Refer to the `conf/` folder on Kaiaulu's git repository for Geronimo and other project configuration files. It is in this project configuration file we specify where Kaiaulu can find the jira sources from Geronimo. We will use the issue_tracker -> jira fields only. In regards to the "issues" and "issue_comments" fields, these should be set to paths where you want to store the jira data. Then, you can access this jira data later using these same paths.

# your_api_token is your atlassian token that is used as your password. Your username is your atlassian username (usually your email). These can be created by navigating to the indicated directories and creating a plain text file (vim atlassian_username or vim atlassian_token) and typing in your username or token. Details on how to obtain an atlassian token can be found here: https://support.atlassian.com/atlassian-account/docs/manage-api-tokens-for-your-atlassian-account/

```{r}
conf <- yaml::read_yaml("../conf/geronimo.yml")
issue_tracker_domain <- conf[["issue_tracker"]][["jira"]][["domain"]]
issue_tracker_project_key <- conf[["issue_tracker"]][["jira"]][["project_key"]]
save_path_issue_tracker_issues <- conf[["issue_tracker"]][["jira"]][["issues"]]
save_path_issue_tracker_issue_comments <- conf[["issue_tracker"]][["jira"]][["issue_comments"]]
your_api_token <- scan("~/.ssh/atlassian_token",what="character",quiet=TRUE)
username <- scan("~/atlassian_username",what="character",quiet=TRUE)

```


# Define a function for pulling issue data from JIRA API.

```{r eval=FALSE}
#' @title Define function to fetch issues from JIRA REST API and save as JSON
#' @description Makes a request to JIRA's latest REST API to retrieve all
#' the necessary information regarding the JIRA issues and downloand
#' them to path designated save_path_issue_tracker_issues
#' @param username is your atlassian username as a string
#' @param password this is your atlassian token
#' @param domain Custom JIRA domain URL . we append the JIRA API endpoint to this. The original is set in the relevant config file
#' @param jql_query the specific query string to specify criteria for fetching
#' @param fields the list of fields that are downloaded in each issue
#' @param maxResults (optional) the maximum number of results to download per page
#' defaul is 50
#' @param verbose boolean flag to specify printing operational 
#' messages or not
fetch_and_save_jira_issues <- function(domain, 
                                       username = NULL,
                                       password = NULL,
                                       jql_query, 
                                       fields, 
                                       save_path_issue_tracker_issues,  
                                       maxResults = 50, 
                                       verbose = FALSE) {
  
  # Ensure the domain starts with https:// for secure communication.
  if (!grepl("^https?://", domain)) {
    domain <- paste0("https://", domain)
  }
  
  # Initialize variables for pagination. startAt variable can be adjusted to be the most recent issue downloaded for the future refresh function.
  startAt <- 0
  total <- maxResults
  all_issues <- list()
  
  repeat{
    # Construct the API endpoint URL
    url <- paste0(domain, "/rest/api/latest/search")
    
     # Authenticate if username and password are provided
  if(!is.null(username) && !is.null(password)){
    auth <- httr::authenticate(as.character(username), as.character(password), "basic")
  } else {
    auth <- NULL
  }
    
    # Prepare query parameters for the API call
    query_params <- list(jql = jql_query, fields = paste(fields, collapse = ","), maxResults = maxResults, startAt = startAt)
    
    # Make the API call
    response <- httr::GET(url, query = query_params)
    
    # Stop if there's an HTTP error
    if (httr::http_error(response)) {
      stop("API request failed: ", httr::http_status(response)$message)
    }
    
    # Extract issues. Append the new issues to all_issues
    content <- jsonlite::fromJSON(httr::content(response, "text", encoding = "UTF-8"), simplifyVector = FALSE)
    all_issues <- append(all_issues, content$issues)
    
    
     # Save all the issues to the specified path
  jsonlite::write_json(all_issues, save_path_issue_tracker_issues)
  
  
    # Update startAt for the next page of results. maxResults is the amount of issues downloaded per loop call. When length(content$issues) < maxResults, this indicates that there are less issues than that remaining to be downloaded and so the loop will break. The second or boolean is for testing and should be deleted before using for serious.
    if ((length(content$issues) < maxResults) || (length(all_issues) > 30)){
      break
      } else {
    startAt <- startAt + maxResults
      }
    
    # Verbose output for each page
    if (verbose) {
      message("Fetched", maxResults, "issues. Total fetched: ", length(all_issues))
    }
  }
  
  # Save all the issues to the specified path
  #jsonlite::write_json(all_issues, save_path_issue_tracker_issues)
  
  # Final verbose output
  if (verbose) {
    message("Fetched and saved a total of ", length(all_issues), " issues to '", save_path_issue_tracker_issues, "'.")
  }
  
  # Returns the content so that it can be saved to a variable via function call
  return(all_issues)
}


```

# Define the variables required for the function call. This chunk also specifies the fields to be downloaded
# Call the function fetch_and_save_jira_issues
```{r eval=FALSE}
# Call the function and save the returned content to variable called all_issues
all_issues <- fetch_and_save_jira_issues(issue_tracker_domain,
                           username, 
                           your_api_token, 
                           jql_query = paste0("project='",issue_tracker_project_key,"'"), 
                           fields = c("summary", 
                           "description",
                           "creator",
                           "assignee",
                           "reporter",
                           "issuetype",
                           "status",
                           "resolution",
                           "components",
                           "created",
                           "updated",
                           "resolutiondate"), 
                           save_path_issue_tracker_issues, 
                           maxResults = 5, 
                           verbose = TRUE)

# save all_issues as a json file along the path save_path_issue_tracker_issues
jsonlite::write_json(all_issues, save_path_issue_tracker_issues)

```

# Download Issue with Comments

In the same manner as before, we can perform the same function call, but including the field `comment`. This will result in the same table being generated but with the additional comment information per issue (if an issue has more than one comment, the issue id is repeated for each different comment). The comment is shown on the column `comment_comments_id`. 

The data of this table can be used to calculate `social smell metrics`, as it represents a form of developer communication. A notebook discussing how to use JIRA data as communication network and/or combining to mailing list data will be made available in the future. 

Since this time around we requested the issue data and comments, when using the `parse_jira` function, both the issues and comments table will be available from the parser. Since the issue table was already displayed, the following show a few rows of the issue comments table:

# Call the function fetch_and_save_jira_issues (defined on line 55)
```{r eval=FALSE}
# Call the function
all_issues_comments <- fetch_and_save_jira_issues(issue_tracker_domain,
                           username, 
                           your_api_token, 
                           jql_query = paste0("project='",issue_tracker_project_key,"'"), 
                           fields = c("summary",
                           "description",
                           "creator",
                           "assignee",
                           "reporter",
                           "issuetype",
                           "status",
                           "resolution",
                           "components",
                           "created",
                           "updated",
                           "resolutiondate",
                           "comment"), 
                           save_path_issue_tracker_issues, 
                           maxResults = 5, 
                           verbose = TRUE)
#path <- file.path(save_path_issue_tracker_issue_comments)
jsonlite::write_json(all_issues_comments, save_path_issue_tracker_issue_comments)

```


#Don't run this because the parser is yet to be built
```{r eval=FALSE}
jira_issue_comments <- parse_jira(save_path_issue_tracker_issue_comments)
jira_issues <- jira_issue_comments[["issues"]]
jira_comments <- jira_issue_comments[["comments"]]
kable(jira_comments[10:11])
```


